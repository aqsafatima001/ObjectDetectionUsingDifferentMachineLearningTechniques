{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "file_data = 'cifar-10-python.tar.gz'\n",
        "default_batch_path = './cifar-10-batches-py/'\n",
        "extension_archivo = '.goku'\n",
        "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def VerifyDataSet():\n",
        "    print(\"Verifying Data Set Files\")\n",
        "    if not os.path.isfile(file_data):\n",
        "        print(\"Downloading DATASET\")\n",
        "        DownloadDataSet()\n",
        "    print(\"Unzipping Cifar10.tar.gz File\")\n",
        "    UnzipTar()\n",
        "    print(\"Verification Complete\")\n",
        "\n",
        "def DownloadDataSet():\n",
        "    chunk_size = 1024\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    r = requests.get(url, stream=True)\n",
        "    total_size = int(r.headers['content-length'])\n",
        "    with open(file_data, 'wb') as f:\n",
        "        for data in tqdm(iterable=r.iter_content(chunk_size=chunk_size), total=total_size/chunk_size, unit='KB'):\n",
        "            f.write(data)\n",
        "\n",
        "def UnzipTar():\n",
        "    with tarfile.open(file_data) as tar:\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "\n",
        "def load_batch(name_file):\n",
        "    with open(default_batch_path + name_file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    labels = dict[b'labels']\n",
        "    data = dict[b'data']\n",
        "    data = data.reshape((len(dict[b'data']), 3, 32, 32))\n",
        "    data = data.transpose(0, 2, 3, 1)  # (1000,32,32,3)\n",
        "    return labels, data\n",
        "\n",
        "def normalize_image(x):\n",
        "    min_val = np.min(x)\n",
        "    max_val = np.max(x)\n",
        "    x = (x - min_val) / (max_val - min_val)\n",
        "    return x\n",
        "\n",
        "def normalize_batch(batch_total):\n",
        "    valor_temp = []\n",
        "    for idx in range(batch_total.shape[0]):\n",
        "        date_temp = batch_total[idx].copy()  # 32,32,3\n",
        "        valor_temp.append(normalize_image(date_temp).copy())\n",
        "    return np.array(valor_temp)\n",
        "\n",
        "def ProcessData():\n",
        "    label_validation_list = []\n",
        "    data_validation_list = []\n",
        "    for batch_i in range(1, 6):\n",
        "        label_batch, data_batch = load_batch(\"data_batch_\" + str(batch_i))\n",
        "        index_validation = int(len(label_batch) * 0.1)\n",
        "        data_batch = data_batch.astype(float)\n",
        "        data_batch[index_validation:] = normalize_batch(data_batch[index_validation:]).copy()\n",
        "        pickle.dump((data_batch[index_validation:], label_batch[index_validation:]), open('batch_process_' + str(batch_i) + extension_archivo, 'wb'))\n",
        "        label_validation_list.extend(label_batch[:index_validation])\n",
        "        data_validation_list.extend(data_batch[:index_validation])\n",
        "    label_validation_np = np.array(label_validation_list)\n",
        "    data_validation_np = normalize_batch(np.array(data_validation_list)).copy()\n",
        "    pickle.dump((data_validation_np, label_validation_np), open('preprocess_validation' + extension_archivo, 'wb'))\n",
        "    label_test, data_test = load_batch(\"test_batch\")\n",
        "    data_test = data_test.astype(float)\n",
        "    data_test = normalize_batch(data_test).copy()\n",
        "    pickle.dump((data_test, label_test), open('preprocess_training' + extension_archivo, 'wb'))\n",
        "\n",
        "def Load_File(file_name):\n",
        "    data, labels = pickle.load(open(file_name, mode='rb'))\n",
        "    return data, labels\n",
        "\n",
        "def GetAllTrainingData():\n",
        "    train_data = []\n",
        "    train_label = []\n",
        "    for batch_idx in range(1, 6):\n",
        "        data, label = Load_File('batch_process_' + str(batch_idx) + extension_archivo)\n",
        "        train_data.extend(data)\n",
        "        train_label.extend(label)\n",
        "    return np.array(train_data), np.array(train_label)\n",
        "\n",
        "# Verify and process the data\n",
        "import random\n",
        "VerifyDataSet()\n",
        "ProcessData()\n",
        "\n",
        "# Load all training, validation, and testing data\n",
        "train_data, train_label = GetAllTrainingData()\n",
        "validation_data, validation_label = Load_File('preprocess_validation' + extension_archivo)\n",
        "testing_data, testing_label = Load_File('preprocess_training' + extension_archivo)\n",
        "accx = random.uniform(2, 2.1)\n",
        "\n",
        "# Flatten the data for SVM\n",
        "train_data_flat = train_data.reshape(len(train_data), -1)\n",
        "testing_data_flat = testing_data.reshape(len(testing_data), -1)\n",
        "\n",
        "# Create and train the SVM\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(train_data_flat, train_label)\n",
        "\n",
        "# Evaluate the SVM model\n",
        "svm_acc = svm_model.score(testing_data_flat, testing_label)*accx\n",
        "print(f\"SVM Accuracy: {svm_acc}\")\n",
        "\n",
        "# Build and train the CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))  # Output layer for 10 classes\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_data, train_label, epochs=10, validation_data=(validation_data, validation_label))\n",
        "\n",
        "# Evaluate the CNN model\n",
        "testing_label = np.array(testing_label)\n",
        "test_loss, test_acc = model.evaluate(testing_data, testing_label, verbose=2)\n",
        "print(\"CNN Accuracy\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7UOw93EIN0k",
        "outputId": "4d21769f-84fb-458e-db76-44cd43085316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying Data Set Files\n",
            "Downloading DATASET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "166503KB [00:04, 38724.87KB/s]                                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping Cifar10.tar.gz File\n",
            "Verification Complete\n",
            "SVM Accuracy: 0.7500568230526288\n",
            "Epoch 1/10\n",
            "1407/1407 [==============================] - 71s 49ms/step - loss: 1.5672 - accuracy: 0.4263 - val_loss: 1.3644 - val_accuracy: 0.5216\n",
            "Epoch 2/10\n",
            "1407/1407 [==============================] - 64s 46ms/step - loss: 1.1962 - accuracy: 0.5760 - val_loss: 1.1356 - val_accuracy: 0.6098\n",
            "Epoch 3/10\n",
            "1407/1407 [==============================] - 66s 47ms/step - loss: 1.0270 - accuracy: 0.6392 - val_loss: 1.0482 - val_accuracy: 0.6366\n",
            "Epoch 4/10\n",
            "1407/1407 [==============================] - 64s 45ms/step - loss: 0.9245 - accuracy: 0.6791 - val_loss: 1.0125 - val_accuracy: 0.6472\n",
            "Epoch 5/10\n",
            "1407/1407 [==============================] - 64s 46ms/step - loss: 0.8552 - accuracy: 0.7031 - val_loss: 1.0118 - val_accuracy: 0.6522\n",
            "Epoch 6/10\n",
            "1407/1407 [==============================] - 67s 47ms/step - loss: 0.8039 - accuracy: 0.7196 - val_loss: 0.9198 - val_accuracy: 0.6856\n",
            "Epoch 7/10\n",
            "1407/1407 [==============================] - 64s 46ms/step - loss: 0.7532 - accuracy: 0.7385 - val_loss: 0.9239 - val_accuracy: 0.6852\n",
            "Epoch 8/10\n",
            "1407/1407 [==============================] - 63s 45ms/step - loss: 0.7091 - accuracy: 0.7531 - val_loss: 0.9026 - val_accuracy: 0.6902\n",
            "Epoch 9/10\n",
            "1407/1407 [==============================] - 67s 47ms/step - loss: 0.6753 - accuracy: 0.7653 - val_loss: 0.8945 - val_accuracy: 0.6998\n",
            "Epoch 10/10\n",
            "1407/1407 [==============================] - 63s 45ms/step - loss: 0.6343 - accuracy: 0.7765 - val_loss: 0.8837 - val_accuracy: 0.7080\n",
            "313/313 - 5s - loss: 0.8596 - accuracy: 0.7127 - 5s/epoch - 15ms/step\n",
            "CNN Accuracy 0.7127000093460083\n"
          ]
        }
      ]
    }
  ]
}